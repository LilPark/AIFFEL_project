{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "instrumental-mexican",
   "metadata": {},
   "source": [
    "# Sequence to sequence learning for performing number addition\n",
    "\n",
    "## 목차\n",
    "1. Introduction\n",
    "2. Setup\n",
    "3. Generate the data\n",
    "4. Vectorize the data\n",
    "5. Build the model\n",
    "6. Train the model\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "* 문자열로 이루어진 두 개의 숫자를 입력받고, 모델을 통해서 두 숫자의 합을 문자열로 예측하는 예제\n",
    "* 예시: Input: \"535+61\", Output: \"596\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-thong",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "generic-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 3\n",
    "REVERSE = True\n",
    "\n",
    "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
    "# int is DIGITS.\n",
    "MAXLEN = DIGITS + 1 + DIGITS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-south",
   "metadata": {},
   "source": [
    "* TRAINING_SIZE: model을 학습할 때 진행할 훈련 횟수\n",
    "* DIGITS: 해당 예제에서 덧셈을 진행할 숫자는 (3자리 이하 숫자) + (3자리 이하 숫자)\n",
    "* REVERSE: \n",
    "* MAXLEN: 입력으로 받을 문자열의 경우, (3자리 이하 숫자)+(3자리 이하 숫자) 이므로, 최대 문자열의 길이는 3+1+3=7 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-modem",
   "metadata": {},
   "source": [
    "## 3. Generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "advance-ukraine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total questions: 50000\n"
     ]
    }
   ],
   "source": [
    "class CharacterTable:\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one-hot integer representation\n",
    "    + Decode the one-hot or integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One-hot encode given string C.\n",
    "        # Arguments\n",
    "            C: string, to be encoded.\n",
    "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        \"\"\"Decode the given vector or 2D array to their character output.\n",
    "        # Arguments\n",
    "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
    "                or a vector of character indices (used with `calc_argmax=False`).\n",
    "            calc_argmax: Whether to find the character index with maximum\n",
    "                probability, defaults to `True`.\n",
    "        \"\"\"\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return \"\".join(self.indices_char[x] for x in x)\n",
    "\n",
    "\n",
    "# All the numbers, plus sign and space for padding.\n",
    "chars = \"0123456789+ \"\n",
    "ctable = CharacterTable(chars)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print(\"Generating data...\")\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(\n",
    "        \"\".join(\n",
    "            np.random.choice(list(\"0123456789\"))\n",
    "            for i in range(np.random.randint(1, DIGITS + 1))\n",
    "        )\n",
    "    )\n",
    "    a, b = f(), f()\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = \"{}+{}\".format(a, b)\n",
    "    query = q + \" \" * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    # Answers can be of maximum size DIGITS + 1.\n",
    "    ans += \" \" * (DIGITS + 1 - len(ans))\n",
    "    if REVERSE:\n",
    "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
    "        # space used for padding.)\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print(\"Total questions:\", len(questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-upgrade",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-termination",
   "metadata": {},
   "source": [
    "## 4. Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "previous-powder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "Training Data:\n",
      "(45000, 7, 12)\n",
      "(45000, 4, 12)\n",
      "Validation Data:\n",
      "(5000, 7, 12)\n",
      "(5000, 4, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"Vectorization...\")\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
    "\n",
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
    "# digits.\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print(\"Training Data:\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"Validation Data:\")\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-theorem",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "casual-catalog",
   "metadata": {},
   "source": [
    "## 5. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comfortable-agreement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128)               72192     \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 4, 128)            131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4, 12)             1548      \n",
      "=================================================================\n",
      "Total params: 205,324\n",
      "Trainable params: 205,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Build model...\")\n",
    "num_layers = 1  # Try to add more LSTM layers!\n",
    "\n",
    "model = keras.Sequential()\n",
    "# \"Encode\" the input sequence using a LSTM, producing an output of size 128.\n",
    "# Note: In a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, num_feature).\n",
    "model.add(layers.LSTM(128, input_shape=(MAXLEN, len(chars))))\n",
    "# As the decoder RNN's input, repeatedly provide with the last output of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "model.add(layers.RepeatVector(DIGITS + 1))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(num_layers):\n",
    "    # By setting return_sequences to True, return not only the last output but\n",
    "    # all the outputs so far in the form of (num_samples, timesteps,\n",
    "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "    # the first dimension to be the timesteps.\n",
    "    model.add(layers.LSTM(128, return_sequences=True))\n",
    "\n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "model.add(layers.Dense(len(chars), activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-signal",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "relative-hurricane",
   "metadata": {},
   "source": [
    "## 6. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "important-titanium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 1\n",
      "1407/1407 [==============================] - 15s 8ms/step - loss: 1.8821 - accuracy: 0.3244 - val_loss: 1.6160 - val_accuracy: 0.3995\n",
      "Q 729+244 T 973  ☒ 916 \n",
      "Q 555+61  T 616  ☒ 666 \n",
      "Q 0+461   T 461  ☒ 567 \n",
      "Q 538+79  T 617  ☒ 483 \n",
      "Q 270+161 T 431  ☒ 366 \n",
      "Q 530+390 T 920  ☒ 516 \n",
      "Q 5+383   T 388  ☒ 888 \n",
      "Q 870+77  T 947  ☒ 883 \n",
      "Q 713+50  T 763  ☒ 787 \n",
      "Q 48+31   T 79   ☒ 11  \n",
      "\n",
      "Iteration 2\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.3463 - accuracy: 0.4966 - val_loss: 1.1563 - val_accuracy: 0.5670\n",
      "Q 815+426 T 1241 ☒ 1182\n",
      "Q 376+505 T 881  ☒ 802 \n",
      "Q 197+84  T 281  ☒ 225 \n",
      "Q 33+78   T 111  ☒ 125 \n",
      "Q 634+685 T 1319 ☒ 1312\n",
      "Q 2+653   T 655  ☒ 669 \n",
      "Q 21+185  T 206  ☒ 292 \n",
      "Q 442+75  T 517  ☒ 522 \n",
      "Q 915+6   T 921  ☒ 929 \n",
      "Q 37+440  T 477  ☒ 479 \n",
      "\n",
      "Iteration 3\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0328 - accuracy: 0.6152 - val_loss: 0.9255 - val_accuracy: 0.6503\n",
      "Q 354+5   T 359  ☒ 352 \n",
      "Q 0+951   T 951  ☒ 949 \n",
      "Q 26+241  T 267  ☑ 267 \n",
      "Q 538+753 T 1291 ☒ 1287\n",
      "Q 2+653   T 655  ☒ 659 \n",
      "Q 37+17   T 54   ☒ 48  \n",
      "Q 964+38  T 1002 ☒ 1001\n",
      "Q 80+660  T 740  ☒ 731 \n",
      "Q 946+2   T 948  ☒ 941 \n",
      "Q 257+284 T 541  ☒ 537 \n",
      "\n",
      "Iteration 4\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.8496 - accuracy: 0.6871 - val_loss: 0.8198 - val_accuracy: 0.6916\n",
      "Q 186+589 T 775  ☒ 770 \n",
      "Q 312+16  T 328  ☒ 326 \n",
      "Q 72+204  T 276  ☒ 271 \n",
      "Q 538+2   T 540  ☒ 539 \n",
      "Q 52+75   T 127  ☒ 120 \n",
      "Q 54+205  T 259  ☒ 261 \n",
      "Q 207+11  T 218  ☒ 217 \n",
      "Q 652+4   T 656  ☒ 651 \n",
      "Q 376+822 T 1198 ☒ 1291\n",
      "Q 76+52   T 128  ☒ 120 \n",
      "\n",
      "Iteration 5\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.7672 - accuracy: 0.7179 - val_loss: 0.7523 - val_accuracy: 0.7184\n",
      "Q 64+373  T 437  ☒ 439 \n",
      "Q 625+79  T 704  ☒ 709 \n",
      "Q 326+696 T 1022 ☒ 1015\n",
      "Q 150+3   T 153  ☒ 156 \n",
      "Q 316+14  T 330  ☒ 337 \n",
      "Q 711+7   T 718  ☑ 718 \n",
      "Q 408+310 T 718  ☒ 810 \n",
      "Q 3+49    T 52   ☒ 59  \n",
      "Q 54+745  T 799  ☒ 790 \n",
      "Q 75+832  T 907  ☒ 909 \n",
      "\n",
      "Iteration 6\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.7043 - accuracy: 0.7426 - val_loss: 0.6935 - val_accuracy: 0.7427\n",
      "Q 372+610 T 982  ☒ 983 \n",
      "Q 17+428  T 445  ☒ 459 \n",
      "Q 1+225   T 226  ☑ 226 \n",
      "Q 386+2   T 388  ☒ 380 \n",
      "Q 808+5   T 813  ☒ 816 \n",
      "Q 674+500 T 1174 ☒ 1177\n",
      "Q 262+281 T 543  ☑ 543 \n",
      "Q 862+389 T 1251 ☑ 1251\n",
      "Q 117+206 T 323  ☒ 327 \n",
      "Q 362+2   T 364  ☑ 364 \n",
      "\n",
      "Iteration 7\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.6088 - accuracy: 0.7760 - val_loss: 0.5059 - val_accuracy: 0.8051\n",
      "Q 553+401 T 954  ☒ 1054\n",
      "Q 595+10  T 605  ☑ 605 \n",
      "Q 452+741 T 1193 ☒ 1194\n",
      "Q 193+33  T 226  ☒ 225 \n",
      "Q 936+296 T 1232 ☒ 1234\n",
      "Q 985+84  T 1069 ☒ 1064\n",
      "Q 346+197 T 543  ☑ 543 \n",
      "Q 42+738  T 780  ☑ 780 \n",
      "Q 374+487 T 861  ☒ 863 \n",
      "Q 53+274  T 327  ☒ 325 \n",
      "\n",
      "Iteration 8\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.3436 - accuracy: 0.8798 - val_loss: 0.2132 - val_accuracy: 0.9434\n",
      "Q 95+63   T 158  ☑ 158 \n",
      "Q 12+264  T 276  ☑ 276 \n",
      "Q 819+65  T 884  ☑ 884 \n",
      "Q 43+43   T 86   ☑ 86  \n",
      "Q 183+19  T 202  ☑ 202 \n",
      "Q 706+8   T 714  ☒ 715 \n",
      "Q 32+48   T 80   ☒ 81  \n",
      "Q 196+775 T 971  ☒ 991 \n",
      "Q 296+67  T 363  ☑ 363 \n",
      "Q 993+9   T 1002 ☒ 1001\n",
      "\n",
      "Iteration 9\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.1663 - accuracy: 0.9560 - val_loss: 0.1399 - val_accuracy: 0.9627\n",
      "Q 808+5   T 813  ☑ 813 \n",
      "Q 966+71  T 1037 ☑ 1037\n",
      "Q 117+670 T 787  ☑ 787 \n",
      "Q 43+79   T 122  ☑ 122 \n",
      "Q 654+45  T 699  ☑ 699 \n",
      "Q 54+348  T 402  ☑ 402 \n",
      "Q 3+968   T 971  ☑ 971 \n",
      "Q 934+592 T 1526 ☑ 1526\n",
      "Q 174+60  T 234  ☑ 234 \n",
      "Q 473+116 T 589  ☑ 589 \n",
      "\n",
      "Iteration 10\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0885 - accuracy: 0.9797 - val_loss: 0.1064 - val_accuracy: 0.9649\n",
      "Q 46+71   T 117  ☑ 117 \n",
      "Q 262+82  T 344  ☑ 344 \n",
      "Q 19+48   T 67   ☑ 67  \n",
      "Q 259+51  T 310  ☑ 310 \n",
      "Q 6+645   T 651  ☑ 651 \n",
      "Q 106+134 T 240  ☑ 240 \n",
      "Q 296+59  T 355  ☑ 355 \n",
      "Q 18+348  T 366  ☑ 366 \n",
      "Q 633+45  T 678  ☑ 678 \n",
      "Q 774+30  T 804  ☑ 804 \n",
      "\n",
      "Iteration 11\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0730 - accuracy: 0.9808 - val_loss: 0.0599 - val_accuracy: 0.9829\n",
      "Q 131+61  T 192  ☑ 192 \n",
      "Q 2+521   T 523  ☑ 523 \n",
      "Q 372+15  T 387  ☑ 387 \n",
      "Q 613+76  T 689  ☑ 689 \n",
      "Q 816+526 T 1342 ☑ 1342\n",
      "Q 49+729  T 778  ☑ 778 \n",
      "Q 77+281  T 358  ☑ 358 \n",
      "Q 179+91  T 270  ☑ 270 \n",
      "Q 987+32  T 1019 ☑ 1019\n",
      "Q 17+99   T 116  ☑ 116 \n",
      "\n",
      "Iteration 12\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0506 - accuracy: 0.9871 - val_loss: 0.0634 - val_accuracy: 0.9794\n",
      "Q 804+36  T 840  ☑ 840 \n",
      "Q 26+670  T 696  ☑ 696 \n",
      "Q 9+514   T 523  ☑ 523 \n",
      "Q 118+469 T 587  ☑ 587 \n",
      "Q 17+900  T 917  ☒ 916 \n",
      "Q 967+489 T 1456 ☑ 1456\n",
      "Q 324+56  T 380  ☑ 380 \n",
      "Q 337+44  T 381  ☑ 381 \n",
      "Q 406+451 T 857  ☑ 857 \n",
      "Q 317+68  T 385  ☑ 385 \n",
      "\n",
      "Iteration 13\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0462 - accuracy: 0.9872 - val_loss: 0.0538 - val_accuracy: 0.9825\n",
      "Q 45+699  T 744  ☑ 744 \n",
      "Q 79+796  T 875  ☑ 875 \n",
      "Q 638+24  T 662  ☑ 662 \n",
      "Q 834+613 T 1447 ☑ 1447\n",
      "Q 16+637  T 653  ☒ 663 \n",
      "Q 139+82  T 221  ☑ 221 \n",
      "Q 757+81  T 838  ☑ 838 \n",
      "Q 24+278  T 302  ☑ 302 \n",
      "Q 66+12   T 78   ☑ 78  \n",
      "Q 725+460 T 1185 ☑ 1185\n",
      "\n",
      "Iteration 14\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0285 - accuracy: 0.9930 - val_loss: 0.0684 - val_accuracy: 0.9779\n",
      "Q 785+5   T 790  ☑ 790 \n",
      "Q 474+4   T 478  ☑ 478 \n",
      "Q 340+42  T 382  ☑ 382 \n",
      "Q 15+954  T 969  ☒ 979 \n",
      "Q 3+490   T 493  ☑ 493 \n",
      "Q 287+7   T 294  ☑ 294 \n",
      "Q 21+85   T 106  ☑ 106 \n",
      "Q 74+600  T 674  ☑ 674 \n",
      "Q 96+672  T 768  ☑ 768 \n",
      "Q 6+221   T 227  ☑ 227 \n",
      "\n",
      "Iteration 15\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0355 - accuracy: 0.9898 - val_loss: 0.0215 - val_accuracy: 0.9950\n",
      "Q 464+64  T 528  ☑ 528 \n",
      "Q 387+972 T 1359 ☑ 1359\n",
      "Q 657+838 T 1495 ☑ 1495\n",
      "Q 298+75  T 373  ☑ 373 \n",
      "Q 4+258   T 262  ☑ 262 \n",
      "Q 406+358 T 764  ☑ 764 \n",
      "Q 860+26  T 886  ☑ 886 \n",
      "Q 86+256  T 342  ☑ 342 \n",
      "Q 672+4   T 676  ☑ 676 \n",
      "Q 644+60  T 704  ☑ 704 \n",
      "\n",
      "Iteration 16\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0320 - accuracy: 0.9911 - val_loss: 0.0304 - val_accuracy: 0.9905\n",
      "Q 58+994  T 1052 ☑ 1052\n",
      "Q 651+73  T 724  ☑ 724 \n",
      "Q 37+17   T 54   ☑ 54  \n",
      "Q 381+37  T 418  ☑ 418 \n",
      "Q 55+14   T 69   ☑ 69  \n",
      "Q 539+72  T 611  ☑ 611 \n",
      "Q 87+461  T 548  ☒ 549 \n",
      "Q 981+416 T 1397 ☑ 1397\n",
      "Q 45+699  T 744  ☑ 744 \n",
      "Q 12+893  T 905  ☑ 905 \n",
      "\n",
      "Iteration 17\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0242 - accuracy: 0.9932 - val_loss: 0.0381 - val_accuracy: 0.9884\n",
      "Q 14+699  T 713  ☑ 713 \n",
      "Q 3+281   T 284  ☑ 284 \n",
      "Q 550+535 T 1085 ☒ 1075\n",
      "Q 36+874  T 910  ☑ 910 \n",
      "Q 215+511 T 726  ☑ 726 \n",
      "Q 888+868 T 1756 ☑ 1756\n",
      "Q 454+10  T 464  ☑ 464 \n",
      "Q 55+395  T 450  ☑ 450 \n",
      "Q 870+313 T 1183 ☑ 1183\n",
      "Q 54+745  T 799  ☑ 799 \n",
      "\n",
      "Iteration 18\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0221 - accuracy: 0.9939 - val_loss: 0.0181 - val_accuracy: 0.9948\n",
      "Q 736+47  T 783  ☑ 783 \n",
      "Q 11+890  T 901  ☑ 901 \n",
      "Q 89+736  T 825  ☑ 825 \n",
      "Q 860+26  T 886  ☑ 886 \n",
      "Q 769+186 T 955  ☑ 955 \n",
      "Q 9+986   T 995  ☑ 995 \n",
      "Q 774+74  T 848  ☑ 848 \n",
      "Q 40+71   T 111  ☑ 111 \n",
      "Q 479+15  T 494  ☑ 494 \n",
      "Q 43+555  T 598  ☑ 598 \n",
      "\n",
      "Iteration 19\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0240 - accuracy: 0.9932 - val_loss: 0.0121 - val_accuracy: 0.9976\n",
      "Q 179+575 T 754  ☑ 754 \n",
      "Q 54+996  T 1050 ☑ 1050\n",
      "Q 81+831  T 912  ☑ 912 \n",
      "Q 42+901  T 943  ☑ 943 \n",
      "Q 340+26  T 366  ☑ 366 \n",
      "Q 71+387  T 458  ☑ 458 \n",
      "Q 67+529  T 596  ☑ 596 \n",
      "Q 8+288   T 296  ☑ 296 \n",
      "Q 59+9    T 68   ☑ 68  \n",
      "Q 418+773 T 1191 ☑ 1191\n",
      "\n",
      "Iteration 20\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0184 - accuracy: 0.9953 - val_loss: 0.0060 - val_accuracy: 0.9994\n",
      "Q 36+824  T 860  ☑ 860 \n",
      "Q 409+638 T 1047 ☑ 1047\n",
      "Q 638+794 T 1432 ☑ 1432\n",
      "Q 171+98  T 269  ☑ 269 \n",
      "Q 156+25  T 181  ☑ 181 \n",
      "Q 34+64   T 98   ☑ 98  \n",
      "Q 535+133 T 668  ☑ 668 \n",
      "Q 6+954   T 960  ☑ 960 \n",
      "Q 887+88  T 975  ☑ 975 \n",
      "Q 11+800  T 811  ☑ 811 \n",
      "\n",
      "Iteration 21\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0221 - accuracy: 0.9941 - val_loss: 0.0075 - val_accuracy: 0.9985\n",
      "Q 422+854 T 1276 ☑ 1276\n",
      "Q 64+45   T 109  ☑ 109 \n",
      "Q 901+760 T 1661 ☑ 1661\n",
      "Q 71+607  T 678  ☑ 678 \n",
      "Q 878+566 T 1444 ☑ 1444\n",
      "Q 36+259  T 295  ☑ 295 \n",
      "Q 48+405  T 453  ☑ 453 \n",
      "Q 25+34   T 59   ☑ 59  \n",
      "Q 20+181  T 201  ☑ 201 \n",
      "Q 0+974   T 974  ☑ 974 \n",
      "\n",
      "Iteration 22\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0198 - accuracy: 0.9947 - val_loss: 0.0053 - val_accuracy: 0.9994\n",
      "Q 961+3   T 964  ☑ 964 \n",
      "Q 743+2   T 745  ☑ 745 \n",
      "Q 728+81  T 809  ☑ 809 \n",
      "Q 934+852 T 1786 ☑ 1786\n",
      "Q 139+82  T 221  ☑ 221 \n",
      "Q 916+251 T 1167 ☑ 1167\n",
      "Q 826+55  T 881  ☑ 881 \n",
      "Q 2+429   T 431  ☑ 431 \n",
      "Q 914+309 T 1223 ☑ 1223\n",
      "Q 612+83  T 695  ☑ 695 \n",
      "\n",
      "Iteration 23\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0156 - accuracy: 0.9958 - val_loss: 0.0149 - val_accuracy: 0.9960\n",
      "Q 781+12  T 793  ☑ 793 \n",
      "Q 36+131  T 167  ☑ 167 \n",
      "Q 184+8   T 192  ☑ 192 \n",
      "Q 79+47   T 126  ☑ 126 \n",
      "Q 91+41   T 132  ☑ 132 \n",
      "Q 518+137 T 655  ☑ 655 \n",
      "Q 651+4   T 655  ☑ 655 \n",
      "Q 287+7   T 294  ☑ 294 \n",
      "Q 0+934   T 934  ☑ 934 \n",
      "Q 159+18  T 177  ☑ 177 \n",
      "\n",
      "Iteration 24\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0067 - accuracy: 0.9986 - val_loss: 0.0045 - val_accuracy: 0.9991\n",
      "Q 402+264 T 666  ☑ 666 \n",
      "Q 562+986 T 1548 ☑ 1548\n",
      "Q 445+76  T 521  ☑ 521 \n",
      "Q 547+790 T 1337 ☑ 1337\n",
      "Q 37+922  T 959  ☑ 959 \n",
      "Q 75+43   T 118  ☑ 118 \n",
      "Q 886+42  T 928  ☑ 928 \n",
      "Q 936+123 T 1059 ☑ 1059\n",
      "Q 936+9   T 945  ☑ 945 \n",
      "Q 443+43  T 486  ☑ 486 \n",
      "\n",
      "Iteration 25\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0219 - accuracy: 0.9941 - val_loss: 0.0056 - val_accuracy: 0.9990\n",
      "Q 248+73  T 321  ☑ 321 \n",
      "Q 816+526 T 1342 ☑ 1342\n",
      "Q 232+978 T 1210 ☑ 1210\n",
      "Q 256+826 T 1082 ☑ 1082\n",
      "Q 633+408 T 1041 ☑ 1041\n",
      "Q 94+759  T 853  ☑ 853 \n",
      "Q 496+30  T 526  ☑ 526 \n",
      "Q 807+38  T 845  ☑ 845 \n",
      "Q 438+622 T 1060 ☑ 1060\n",
      "Q 179+934 T 1113 ☑ 1113\n",
      "\n",
      "Iteration 26\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0177 - accuracy: 0.9952 - val_loss: 0.0117 - val_accuracy: 0.9965\n",
      "Q 403+69  T 472  ☑ 472 \n",
      "Q 337+51  T 388  ☑ 388 \n",
      "Q 162+1   T 163  ☑ 163 \n",
      "Q 644+469 T 1113 ☑ 1113\n",
      "Q 888+50  T 938  ☑ 938 \n",
      "Q 601+202 T 803  ☑ 803 \n",
      "Q 8+664   T 672  ☑ 672 \n",
      "Q 117+670 T 787  ☑ 787 \n",
      "Q 269+4   T 273  ☑ 273 \n",
      "Q 236+79  T 315  ☑ 315 \n",
      "\n",
      "Iteration 27\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0149 - accuracy: 0.9957 - val_loss: 0.0084 - val_accuracy: 0.9982\n",
      "Q 867+69  T 936  ☑ 936 \n",
      "Q 862+44  T 906  ☑ 906 \n",
      "Q 19+605  T 624  ☑ 624 \n",
      "Q 87+862  T 949  ☑ 949 \n",
      "Q 265+11  T 276  ☑ 276 \n",
      "Q 724+2   T 726  ☑ 726 \n",
      "Q 169+80  T 249  ☑ 249 \n",
      "Q 274+451 T 725  ☑ 725 \n",
      "Q 63+719  T 782  ☑ 782 \n",
      "Q 107+193 T 300  ☑ 300 \n",
      "\n",
      "Iteration 28\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0120 - accuracy: 0.9966 - val_loss: 0.0060 - val_accuracy: 0.9987\n",
      "Q 436+50  T 486  ☑ 486 \n",
      "Q 938+92  T 1030 ☑ 1030\n",
      "Q 34+565  T 599  ☑ 599 \n",
      "Q 212+681 T 893  ☑ 893 \n",
      "Q 17+91   T 108  ☑ 108 \n",
      "Q 4+231   T 235  ☑ 235 \n",
      "Q 83+346  T 429  ☑ 429 \n",
      "Q 546+631 T 1177 ☑ 1177\n",
      "Q 935+81  T 1016 ☑ 1016\n",
      "Q 274+75  T 349  ☑ 349 \n",
      "\n",
      "Iteration 29\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.0161 - accuracy: 0.9958 - val_loss: 0.0226 - val_accuracy: 0.9930\n",
      "Q 37+17   T 54   ☑ 54  \n",
      "Q 147+1   T 148  ☑ 148 \n",
      "Q 731+98  T 829  ☑ 829 \n",
      "Q 18+45   T 63   ☑ 63  \n",
      "Q 898+474 T 1372 ☑ 1372\n",
      "Q 473+641 T 1114 ☑ 1114\n",
      "Q 4+648   T 652  ☑ 652 \n",
      "Q 139+82  T 221  ☑ 221 \n",
      "Q 324+31  T 355  ☑ 355 \n",
      "Q 95+189  T 284  ☑ 284 \n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Train the model each generation and show predictions against the validation\n",
    "# dataset.\n",
    "for epoch in range(1, epochs):\n",
    "    print()\n",
    "    print(\"Iteration\", epoch)\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=1,\n",
    "        validation_data=(x_val, y_val),\n",
    "    )\n",
    "    # Select 10 samples from the validation set at random so we can visualize\n",
    "    # errors.\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = np.argmax(model.predict(rowx), axis=-1)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print(\"Q\", q[::-1] if REVERSE else q, end=\" \")\n",
    "        print(\"T\", correct, end=\" \")\n",
    "        if correct == guess:\n",
    "            print(\"☑ \" + guess)\n",
    "        else:\n",
    "            print(\"☒ \" + guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-details",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
