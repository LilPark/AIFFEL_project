{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lucky-appraisal",
   "metadata": {},
   "source": [
    "# [E-01]RockPaperScissor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sitting-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-difference",
   "metadata": {},
   "source": [
    "### Why use 'PIL library'?\n",
    "* 가위바위보 데이터 이미지의 크기는 224 X 224.\n",
    "* 데이터 이미지의 크기를 28 x 28 로 줄여야 함.\n",
    "* 이를 위해, PIL 라이브러리를 사용\n",
    "\n",
    "---\n",
    "\n",
    "### glob\n",
    "* 파일들의 리스트를 뽑을 때 사용.\n",
    "* 와일드카드 문자를 사용해서 일정한 패턴을 가진 파일 이름들을 지정하기 위한 패턴\n",
    "* 아래의 resize_images 함수에서 /*.jpg에 해당하는 파일들을 리스트로 images 변수에 저장\n",
    "\n",
    "---\n",
    "### numpy\n",
    "* load_data 함수에서 numpy를 이용하여 이미지 데이터를 담을 행렬을 생성\n",
    "\n",
    "---\n",
    "### matplotlib\n",
    "* training set에 저장되어 있는 이미지의 resize가 잘 이루어졌는지, 이미지에 다른 문제는 없는지 확인하기 위해서, matplotlib을 이용하여 이미지를 출력할 예정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-exception",
   "metadata": {},
   "source": [
    "# 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "spiritual-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(img_path):\n",
    "    images = glob.glob(img_path + \"/*.jpg\")\n",
    "    \n",
    "    print(len(images), \" images to be resized.\")\n",
    "    \n",
    "    target_size = (28, 28)\n",
    "    for img in images:\n",
    "        old_img = Image.open(img)\n",
    "        new_img = old_img.resize(target_size, Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "        \n",
    "    print(len(images), \" images resized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-filling",
   "metadata": {},
   "source": [
    "### resize_images(img_path)\n",
    "\n",
    "1. glob 모듈을 이용하여, 매개변수 img_path 내부에 있는 '/*.jpg' 형식의 모든 파일을 리스트의 형식으로 images 변수에 저장\n",
    "2. resize를 원하는 이미지 크기가 (28 X 28)이므로, target_size 변수에 저장\n",
    "3. 이후 for문을 이용하여 images 리스트에 있는 모든 파일(jpg 파일)의 사이즈를 (28 X 28)로 바꾸고, new_img 라는 변수에 저장 (resize 메소드 사용)\n",
    "4. 이때 사용한 ANTIALIAS는, anti-aliasing의 줄임말로, 높은 해상도의 신호를 낮은 해상도에서 나타낼 때 생기는 계단 현상(깨진 패턴)을 최소화하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sophisticated-modern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500  images to be resized.\n",
      "1500  images resized.\n",
      "Scissor image resized (28 X 28)\n",
      "1500  images to be resized.\n",
      "1500  images resized.\n",
      "Rock image resized (28 X 28)\n",
      "1500  images to be resized.\n",
      "1500  images resized.\n",
      "Paper image resized (28 X 28)\n"
     ]
    }
   ],
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/train/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"Scissor image resized (28 X 28)\")\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/train/rock\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"Rock image resized (28 X 28)\")\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/train/paper\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"Paper image resized (28 X 28)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-divorce",
   "metadata": {},
   "source": [
    "## 이미지 크기 변경\n",
    "* training set으로 사용할 이미지들의 크기를 (28 X 28)로 변경 (가위, 바위, 보)\n",
    "* os.getenv: 환경변수 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "blank-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(img_path, number_of_data=4500):\n",
    "    img_size = 28\n",
    "    color = 3\n",
    "    \n",
    "    imgs = np.zeros(number_of_data * img_size * img_size * color, dtype = np.int32).reshape(number_of_data, img_size, img_size, color)\n",
    "    labels = np.zeros(number_of_data, dtype = np.int32)\n",
    "    \n",
    "    idx = 0\n",
    "    for file in glob.iglob(img_path + '/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file), dtype = np.int32)\n",
    "        imgs[idx, :, :, :] = img\n",
    "        labels[idx] = 0    # scissor: 0\n",
    "        idx = idx + 1\n",
    "    \n",
    "    for file in glob.iglob(img_path + '/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file), dtype = np.int32)\n",
    "        imgs[idx, :, :, :] = img\n",
    "        labels[idx] = 1    # rock: 1\n",
    "        idx = idx + 1\n",
    "        \n",
    "    for file in glob.iglob(img_path + '/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file), dtype = np.int32)\n",
    "        imgs[idx, :, :, :] = img\n",
    "        labels[idx] = 2    # paper: 2\n",
    "        idx = idx + 1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx, \"입니다.\")\n",
    "    return imgs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-glucose",
   "metadata": {},
   "source": [
    "### load_data(img_path, number_of_data=4500)\n",
    "\n",
    "1. img_path 내의 데이터 이미지를 행렬에 저장하는 함수\n",
    "2. 이미지 데이터와 라벨을 행렬에 저장\n",
    "3. 하나의 데이터 셋 안에 이미지 개수는 4500개\n",
    "4. iglob(): 리스트 형식이 아닌 이터레이터 형식으로 반환 (파일이나 폴더 이름을 취득해 for문에서 처리하고 싶은 경우에는 이터레이터 형식으로 취득하도록 iglob()를 사용해야 함)\n",
    "5. dtype=np.int32: array의 데이터 형식을 지정해주는 방법. 정수 단일 값을 32bits 메모리에 저장한다는 의미\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "jewish-circular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 4500 입니다.\n",
      "x_train shape: (4500, 28, 28, 3)\n",
      "y_train shape: (4500,)\n"
     ]
    }
   ],
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/train\"\n",
    "(x_train, y_train) = load_data(image_dir_path)\n",
    "x_train_norm = x_train / 255.0\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-alaska",
   "metadata": {},
   "source": [
    "## load_data 함수를 이용하여 training set 만들기\n",
    "\n",
    "* x_train1에는 학습할 이미지가 저장\n",
    "* y_train1에는 이미지의 라벨을 저장\n",
    "* x_train1_norm: 기존 이미지들의 픽셀 값은 0 ~ 255 사이의 값. 인공지능 모델을 훈련시키고 사용할 때, 일반적으로 입력은 0 ~ 1 사이의 값으로 정규화 시키는 것이 좋음.\n",
    "* 따라서 x_train1에 저장된 이미지들의 픽셀 값을 0 ~ 1 사이의 값으로 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "atlantic-science",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0])\n",
    "print('라벨: ', y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-rental",
   "metadata": {},
   "source": [
    "### 이미지를 resize하는 이유는 무엇일까?\n",
    "가위바위보 분류기를 만들기 전에, 노드에서 숫자를 분류하는 예제를 학습했다.\n",
    "숫자를 분류하는 예제에서는 이미지의 크기가 (28 X 28) 이었다.\n",
    "숫자 이미지는 28 X 28에서도 충분히 숫자를 알아보기 쉬웠고, 흑백 이미지였기 때문에 분류가 쉬운 편이라고 생각한다.\n",
    "하지만 가위바위보 데이터 이미지의 경우, (28 X 28)로 resize 한 결과를 보면, 정확하게 분류하기 힘들다고 생각된다.\n",
    "위 이미지만 보더라도, 언뜻 보면 가위가 아니라 바위처럼 보인다.\n",
    "resize를 하지 않고, 기존 이미지 크기 그대로 사용하면 안 될까?\n",
    "---\n",
    "현 시점에서, resize의 의미가 수행시간의 단축이라고 생각된다. 그렇다면, (28 X 28)의 크기로 줄이는 것이 가장 효율적일까?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-region",
   "metadata": {},
   "source": [
    "# 2. 모델 생성 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "veterinary-segment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 122,051\n",
      "Trainable params: 122,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_channel_1 = 32\n",
    "n_channel_2 = 64\n",
    "n_dense = 64\n",
    "n_train_epoch = 10\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3, 3), activation = 'relu', input_shape = (28, 28, 3)))\n",
    "model.add(keras.layers.MaxPool2D(2, 2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3, 3), activation = 'relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation = 'relu'))\n",
    "model.add(keras.layers.Dense(3, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-register",
   "metadata": {},
   "source": [
    "### 가위바위보를 인식하는 딥러닝 네트워크 설계\n",
    "\n",
    "* 이미지의 특징 수를 32, 64로 설정\n",
    "* 뉴런의 수를 64로 설정\n",
    "* 8개의 층으로 이루어진 model을 구현\n",
    "* Conv2D: 케라스에서 제공하는 Convolution 레이어로, 영상 처리에 주로 사용\n",
    "* MaxPool2D: Convolution 레이어의 출력 이미지에서 주요값만 뽑아 크기가 작은 출력 영상을 만듬. 이를 통해서, 지역적인 사소한 변화가 영향을 미치지 않도록 함\n",
    "* MaxPooling2D: MaxPool2D와 이름만 다를 뿐, 같은 기능을 수행\n",
    "* Flatten: 영상을 1차원으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "third-worker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "141/141 [==============================] - 7s 20ms/step - loss: 1.0410 - accuracy: 0.4299\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 0.6534 - accuracy: 0.7312\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 0.4065 - accuracy: 0.8505\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 0.3069 - accuracy: 0.8915\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 0.2062 - accuracy: 0.9329\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 0.1599 - accuracy: 0.9461\n",
      "Epoch 7/10\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 0.1365 - accuracy: 0.9546\n",
      "Epoch 8/10\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 0.1258 - accuracy: 0.9555\n",
      "Epoch 9/10\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 0.0757 - accuracy: 0.9783\n",
      "Epoch 10/10\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 0.0788 - accuracy: 0.9764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9128508850>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "             loss = 'sparse_categorical_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "model.fit(x_train_norm, y_train, epochs = n_train_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-avenue",
   "metadata": {},
   "source": [
    "### 딥러닝 네트워크 학습\n",
    "* optimizer는 학습에서 실제로 파라미터를 갱신시키는 부분을 의미\n",
    "* adam(Adaptive Moment Estimation): 현재 가장 자주 사용되는 optimizer로, 다른 optimizer들의 장점을 취해 만든 optimizer\n",
    "* loss는 loss function을 의미\n",
    "* sparse_categorical_crossentropy: 다중 분류 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-chicago",
   "metadata": {},
   "source": [
    "# 3. Test set을 이용한 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "amazing-allen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "100  images to be resized.\n",
      "100  images resized.\n"
     ]
    }
   ],
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/rock\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/paper\"\n",
    "resize_images(image_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-details",
   "metadata": {},
   "source": [
    "* test set 이미지 크기 변경 (28 X 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "differential-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(img_path, number_of_data = 300):\n",
    "    img_size = 28\n",
    "    color = 3\n",
    "    \n",
    "    imgs = np.zeros(number_of_data * img_size * img_size * color, dtype = np.int32).reshape(number_of_data, img_size, img_size, color)\n",
    "    labels = np.zeros(number_of_data, dtype = np.int32)\n",
    "    \n",
    "    idx = 0\n",
    "    for file in glob.iglob(img_path + '/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file), dtype = np.int32)\n",
    "        imgs[idx,:,:,:] = img\n",
    "        labels[idx] = 0\n",
    "        idx = idx + 1\n",
    "    \n",
    "    for file in glob.iglob(img_path + '/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file), dtype = np.int32)\n",
    "        imgs[idx,:,:,:] = img\n",
    "        labels[idx] = 1\n",
    "        idx = idx + 1\n",
    "        \n",
    "    for file in glob.iglob(img_path + '/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file), dtype = np.int32)\n",
    "        imgs[idx,:,:,:] = img\n",
    "        labels[idx] = 2\n",
    "        idx = idx + 1\n",
    "        \n",
    "    print(\"테스트데이터(x_test)의 이미지 개수는\", idx, \"입니다.\")\n",
    "    return imgs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-sociology",
   "metadata": {},
   "source": [
    "## load_test_data(img_path, number_of_data=300)\n",
    "* 이전에 만들었던 load_data 함수와 같은 내용\n",
    "* test set의 이미지를 불러오는 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "generous-roots",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트데이터(x_test)의 이미지 개수는 300 입니다.\n",
      "x_test shape: (300, 28, 28, 3)\n",
      "y_test shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test\"\n",
    "(x_test, y_test) = load_test_data(image_dir_path)\n",
    "x_test_norm = x_test / 255.0\n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-mexican",
   "metadata": {},
   "source": [
    "## load_test_data 함수를 이용하여 test set 만들기\n",
    "\n",
    "* test set을 저장하고, 이미지의 픽셀 값을 0 ~ 1 사이의 값으로 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "handmade-champion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 1s - loss: 0.9623 - accuracy: 0.7433\n",
      "test_loss: 0.9623026251792908\n",
      "test_accuracy: 0.7433333396911621\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_norm, y_test, verbose=2)\n",
    "print(\"test_loss: {}\".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-livestock",
   "metadata": {},
   "source": [
    "* verbose: 상세한 logging을 출력할지를 조절하는 parameter\n",
    "* verbose=0: silent, verbose=1: progress bar, verbose=2: one line per epoch\n",
    "* verbose 값이 0보다 크면 로그를 출력하게되기 때문에, 실행 속도에 영향을 줄 수 있음\n",
    "---\n",
    "* test set을 이용하여, 위에서 훈련시킨 model을 평가\n",
    "* model의 loss와 accuracy를 측정\n",
    "* 300개의 training set으로 학습했을때, test_loss: 1.9043, test_accuracy: 0.3333\n",
    "* 600개의 training set으로 학습했을때, test_loss: 2.0036, test_accuracy: 0.4333\n",
    "* 900개의 training set으로 학습했을때, test_loss: 1.1084, test_accuracy: 0.6033\n",
    "* 1200개의 training set으로 학습했을때, test_loss: 1.6996, test_accuracy: 0.5300\n",
    "* 1500개의 training set으로 학습했을때, test_loss: 1.5470, test_accuracy: 0.6433\n",
    "* 4500개의 training set으로 학습했을때, test_loss: 0.9623, test_accuracy: 0.7433"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-district",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# 4. 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-daughter",
   "metadata": {},
   "source": [
    "### * 모델의 정확도는 약 74%\n",
    "\n",
    "### * 모델 성능 향상을 위한 시도\n",
    "    1. Training set의 수를 300개부터 1500개까지, 300개씩 늘려가면서 학습시켰음. training set이 많을수록 accuracy가 증가하다가, 다시 감소하다가 1500개의 training set에서 다시 증가하는 경향을 보였음. 이는 학습 데이터의 양이 많을수록 좋은 것은 사실이나, 학습 데이터의 질도 무시하지 못한다는 것을 의미한다고 생각함. 이후, training set의 양을 1500개에서 4500개로 한번에 늘려서 학습을 진행. 이때도 어느정도 정확한 사진들로 이루어진 training set을 만들기 위해서, 이미지들을 확인하면서 training set에 추가함. 그 결과, 약 74%의 정확도를 측정.\n",
    "    2. Test set도 중요한 영향을 미친다고 생각했기에 퀄리티가 좋은 test set 확보를 위해서, data set을 확인해보고 손모양이 비교적 정확한 data set으로 선정.\n",
    "    3. Hyperparameter 또한 바꿔가면서 학습을 진행함. 물론, hyerparameter도 중요하지만, hyperparameter에 따른 정확도를 비교했을때, training set의 크기나 test set의 질 보다는 중요성이 낮다고 판단."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-association",
   "metadata": {},
   "source": [
    "# 5. 추가적인 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-circumstances",
   "metadata": {},
   "source": [
    "## 모델을 어떻게 학습시켜야 하는가?\n",
    "* 적절한 hyperparameter를 찾는 것은 중요하지만, hyperparameter의 변화로 인한 accuracy의 증가는 한계가 있음\n",
    "* 적절한 hyperparameter를 찾는 것 보다는, 질 좋은 training set을 많이 확보하는 것이 중요하다고 느꼈음\n",
    "* 900개의 training set을 이용하여 학습한 경우, accuracy가 0.6033이었지만, 1200개의 training set을 이용하여 학습한 경우, accuracy가 0.5300으로 감소하였음\n",
    "* 그러나, training set을 1500개로 증가했을때, accuracy가 0.6433으로 다시 증가했기 때문에, training set의 크기로 인해서 accuracy가 감소한 것은 아님.\n",
    "* 따라서, 이미지의 퀄리티(손모양의 정확도)가 accuracy에 영향을 미쳤다고 판단.\n",
    "* 결과적으로, training set을 구성할 때, 최대한 많은 이미지를 사용하여 구성하는 것도 중요하지만, training set을 구성하는 이미지들의 퀄리티 또한 중요하다고 판단."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-wildlife",
   "metadata": {},
   "source": [
    "## 이미지 크기에 따른 accuracy의 변화 (1500개의 이미지 training set 기준)\n",
    "* 기존의 (28 X 28) 이미지가 아닌 (128 X 128) 크기의 이미지를 이용하여 학습한 경우, accuracy: 0.5067\n",
    "* (224 X 224) 크기의 이미지로 학습한 경우, accuracy: 0.4633\n",
    "* 이미지의 크기가 커질수록, 학습에 소요되는 시간이 증가하였음\n",
    "* 그러나 이미지의 크기가 커질수록, accuracy가 감소하는 경향을 보였음\n",
    "* 따라서 이미지의 크기가 크다고해서 무조건적인 accuracy의 증가로 이루어지는 것은 아니라고 판단\n",
    "* 해당 예제에서는 이미지의 크기가 커질수록 accuracy가 감소한다고 보여지지만, 표본이 적기에 무조건적으로 감소한다고 판단하기에는 어려움\n",
    "* 이미지의 크기보단, **질** 좋은 training set을 **많이** 이용하여 학습하는 것이 모델의 accuracy를 확실하게 증가시킬 수 있다고 판단"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-negotiation",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
